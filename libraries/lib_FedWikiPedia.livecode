script "lib_FedWikiPedia"
--> Metadata
-
name: lib_FedWikiPedia
type: library
version: 1.4
copyright: David Bovill
licence:  GPLv3
deps: lib_MediaWiki, lib_Fedwiki

/*
These handlers interface between Wikipedia and Fedwiki.
Contains clean functions for wikipedia import into fedwiki.

== TO DO ==
Everything at the bottom of this code is in process of being moved to it's own library - lib_Encoding


command fedwikipedia_CleanConvertTemplate @wikiSectionLine
   -- some start text {{convert|22|–|25|C|F}} also {{convert|4|C-change|sigfig=1}} end text 
   -- put script_GetCommentAboveMe() into wikiSectionLine
   
   put "[^\}]+" into untilCurlyBracket
   put "\{\{convert\|(" & untilCurlyBracket & ")\}\}" into someReg
   put "(?m)" before someReg -- multiline search
   
   set the itemdelimiter to "|"
   repeat
      if matchchunk (wikiSectionLine, someReg, someStart, someEnd) is true then
         put char someStart to someEnd of wikiSectionLine into someText
         switch the number of items of someText
            case 3
               -- 4|C-change|sigfig=1
               put item 1 of someText into someNum
               put item 2 of someText into someSymbol
               replace "-change" with empty in someSymbol
               put someNum & fromSymbol into cleanText
               break
            default -- case 5
               put item 1 to - 3 of someText into someData
               replace "|" with empty in someData
               put item -2 of someText into fromSymbol
               put item -1 of someText into toSymbol
               put someData & fromSymbol into cleanText
         end switch
         
         subtract 10 from someStart
         add 2 to someEnd
         put cleanText into char someStart to someEnd of wikiSectionLine
      else
         exit repeat
      end if
   end repeat
end fedwikipedia_CleanConvertTemplate

command fedwikipedia_CleanCitations @wikiText
   _StripCitations wikiText
end fedwikipedia_CleanCitations

private command _StripCitations @wikiText
   -- starting text <ref name="fdsa">{{cite web|author=Nisi Shawl |url=http://seattletimes.nwsource.com/html/books/2008758249_br22sterling.html |title=Books &#124; "The Caryatids": four clones need a home &#124; Seattle Times Newspaper |publisher=Seattletimes.nwsource.com |date=2009-02-19 |accessdate=2010-01-01}}</ref> something at the end
   -- put script_GetCommentAboveMe() into wikiSectionLine
   
   put "[^\}]+" into untilCurlyBracket
   put "\{\{cite (" & untilCurlyBracket & ")\}\}" into someReg
   put "(<ref[^\>]*>)" before someReg
   put "</ref>" after someReg
   put "(?mi)" before someReg -- multiline search
   
   text_StripReg wikiText, someReg
   return wikiText
end _StripCitations

private command _ProcessCitation @wikiText, someReg
   repeat
      if matchchunk (wikiText, someReg, refStart, refEnd, wikCiteStart, wikiCiteEnd) is true then
         put char wikCiteStart to wikiCiteEnd of wikiText into wikiCitation
         put _ExtractCitation (wikiCitation) into cleanCitation
         
         put cleanCitation into char refStart to (wikiCiteEnd + 8) of wikiText
      else
         exit repeat
      end if
   end repeat
end _ProcessCitation

private function _ExtractCitation wikiCitation
   split wikiCitation with "|" and "="
   put word 1 to -1 of wikiCitation ["url"] into citationURL
   put word 1 to -1 of wikiCitation ["title"] into citationTitle
   put "[" & citationURL && citationTitle & "]" into cleanCitation
   return cleanCitation
end _ExtractCitation

command fedwikipedia_CleanTable @wikiText
   _CleanTable wikiText
end fedwikipedia_CleanTable

*/


--> Working On
-  
function fedwiki_FetchWikipediaSummaryPageArray pageSlug, pApiStem
   -- use "mediawiki_FetchSummaryPageArray" instead
   -- see also "fedwikipedia_FetchSectionSummaryPageArray"
   -- need to make a "parse" call to get TOC
   local pSectionNum
   
   mediawiki_FetchPageParseInfo pageSlug, pageDescription, pageTitle, shortImageFile, infoBoxArray, pSectionNum, pApiStem
   put the result into sandboxArray
   put fedwikipedia_ConstructPageTocArray (sandboxArray) into tocArray
   
   fedwikipedia_ConstructTocLookupArray tocArray, lookUpArray, markdownTOC
   if lookUpArray is an array then put "https://rest.livecode.world/mediawiki/importSection" into lookUpArray ["default"]["transport"]
   
   -- need to make a "query" call to get a nice clean text
   mediawiki_FetchPageQueryInfo pageSlug, simplePageDescription, pageTitleAgain, shortImageFile, wikipediaRevisionID, pApiStem
   put the result into sandboxPageArray
   
   -- put wikipedia_ConstructUrl (pageSlug) into wikipediaUrl
   put sandboxPageArray ["fullurl"] into wikipediaUrl
   put sandboxPageArray ["coordinates"] into coordArray
   
   put fedwikipedia_ConstructSourceArray (pageTitle, pageSlug, wikipediaUrl, wikipediaRevisionID, lookUpArray) into pSourceArray
   
   -- now let's construct the page
   set the itemdelimiter to "."
   if item -1 of shortImageFile is "webm" then
      put wikicommons_ConstructVideoPageArray (pageTitle, simplePageDescription, wikipediaUrl, shortImageFile, pSourceArray) into fedwikiPageArray
   else
      put wikicommons_ConstructTwoImagePageArray (pageTitle, simplePageDescription, shortImageFile, wikipediaUrl, pSourceArray) into fedwikiPageArray
   end if
   
   if coordArray is an array then
      -- add a map?
   end if
   
   put "# See also" & CR & markdownTOC into markdownSection
   pageArray_AddMarkdown fedwikiPageArray, markdownSection
   
   pageArray_StripJournal fedwikiPageArray
   return fedwikiPageArray
end fedwiki_FetchWikipediaSummaryPageArray


--> Mediawiki | Fetch | Alternative
-
function mediawiki_FetchSummaryPageArray pageSlug, pLang
   -- was "restbase_FetchSummaryPageArray"
   -- see also "fedwiki_FetchWikipediaSummaryPageArray"
   -- very similar to "mediawiki_FetchPageArray"
   
   /*   
   -- consider using "put mediawiki_FetchWikiText("Ant")"
   -- also uses old "mediawiki_FetchSummaryDescription" to fetch multiple first lines
   -- as restbase won't do that except in html
   -- would be great to enable links in first paragraphs like "wikicommons_ConstructSectionPageArray" does
   -- using sectionnum = 0 does not work with "wikicommons_ConstructSectionPageArray"
   */
   
   put mediawiki_FetchSummaryDescription (pageSlug, true, true, pLang) into firstSection -- uses old stuff anyway to get more than one line
   put restbase_FetchSummaryArray (pageSlug, pLang) into summaryArray -- fetch some metadata
   put restbase_FetchMobileSectionArray (pageSlug) into mobileSectionArray
   --
   put mediawiki_ConstructSummaryPageArray (firstSection, summaryArray, mobileSectionArray) into pageArray
   return pageArray
end mediawiki_FetchSummaryPageArray

command mediawiki_FetchPageParseInfo pageSlug, @pageDescription, @pageTitle, @shortImageFile, @infoBoxArray, pSectionNum, pApiStem
   -- Uses "parse" to extract wiktext sections. Lot's of fiddling with text. Messy as it returns lots of page templates that need cleaning. But we get wiki links
   
   if pSectionNum is empty then
      put sandbox_ConstructSectionFragment (pageSlug) into sandBoxFragment
   else
      put sandbox_ConstructSectionFragment (pageSlug, pSectionNum) into sandBoxFragment
   end if
   
   put sandbox_FetchArray (sandBoxFragment, pApiStem) into sandboxArray
   
   put sandboxArray ["parse"] into pageArray
   put pageArray ["title"] into pageTitle
   put pageArray ["wikitext"]["*"] into pageDescription
   mediawiki_CleanPageDescription pageDescription, infoBoxArray
   if infoBoxArray is an array then
      put infoBoxArray ["image"] into shortImageFile
   else
      put pageArray ["images"] into imageArray
      put mediawiki_ExtractFirstInterestingImage (imageArray) into shortImageFile
   end if
   return sandboxArray
end mediawiki_FetchPageParseInfo


--> Fedwiki | Clean
-
/*
These handlers are for parsing, and cleaning mediawiki stypled wikitext, to turn it into a form that is useful for Fedwiki.
*/

command fedwikipedia_CleanWikiText @wikiText
   -- _StripCitations wikiText -- these are always withing "<ref>" tags
   _CleanReferences wikiText
   _CleanSeeAlso wikiText
   _CleanTable wikiText
   _CleanComments wikiText
   _CleanBlockQuotes wikiText
   
   _CleanTemplates wikiText
   
   -- hacks
   replace "'''" with "__" in wikiText -- for bold markdown
   replace "()" with empty in wikiText
   replace "[ ]" with empty in wikiText
   replace "&nbsp;" with empty in wikiText
   replace "to(-)" with " to " in wikiText
   -- replace "[]" with empty in wikiText
   
   replace (CR & "*") with (CR & "* ") in wikiText
   --
   put word 1 to -1 of wikiText into wikiText
end fedwikipedia_CleanWikiText

command fedwikipedia_CleanInternalLinks @wikiSectionLine
   -- used by "wikicommons_AddSectionToPageArray"
   
   -- strips out image files as well (carefull)
   -- some starting text [[Machtergreifung|came to power]] something [[Real Link|well a second one]] at end
   -- Berlin was devastated by [[Bombing of Berlin in World War II|bombing raids]], fires and street battles during 
   -- put script_GetCommentAboveMe() into wikiSectionLine
   
   put "\[\[([^\]]+)\|([^\]]+)\]\]" into someReg
   put "(?m)" before someReg -- multiline search
   repeat
      if matchchunk (wikiSectionLine, someReg, wikiLinkStart, wikiLinkEnd, linktextStart, linktextEnd) is true then
         -- [[Rhaeto-Romans|Romansh language]] => Rhaeto-Romans ([[Romansh language]])
         -- but does not fix section links [[Rhaeto-Romans#Stuff]] and other stuff
         
         put char wikiLinkStart to wikiLinkEnd of wikiSectionLine into wikiLink
         put char linktextStart to linktextEnd of wikiSectionLine into wikiLinkText
         --
         -- put wikiLinkText && "([[" & wikiLink & "]])" into cleanedLink -- a bit too cluttered
         put "[[" & wikiLink & "]]" into cleanedLink
         --
         put cleanedLink into char (wikiLinkStart - 2) to (linktextEnd + 2) of wikiSectionLine
      else
         exit repeat
      end if
   end repeat
end fedwikipedia_CleanInternalLinks


--> FedwikiPedia | Fetch
-   
function fedwiki_FetchWikipediaSummaryPageJson pageSlug, pApiStem
   put fedwiki_FetchWikipediaSummaryPageArray (pageSlug, pApiStem) into fedwikiPageArray
   put json_FromArray (fedwikiPageArray) into pageJSON
   return pageJSON
end fedwiki_FetchWikipediaSummaryPageJson

function fedwikipedia_ConstructPageTocArray sandboxArray
   put sandboxArray ["parse"]["sections"] into sectionArray
   put sectionArray [1]["fromTitle"] into tocTitle
   
   repeat for each key tocNum in sectionArray
      put sectionArray [tocNum]["level"] into tocLevel
      put sectionArray [tocNum]["line"] into tocLine
      put sectionArray [tocNum]["number"] into tocNumber
      
      put tocNumber into tocArray ["index"][tocLine]["dot_number"]
      put tocNum into tocArray ["index"][tocLine]["section_number"]
      
      put tocNumber into dotKeyArray
      split dotKeyArray by "."
      put tocLine into tocArray [dotKeyArray]["meta"]["title"]
      put tocNumber into tocArray [dotKeyArray]["meta"]["dot_number"]
      put tocNum into tocArray [dotKeyArray]["meta"]["section_number"]
      put tocLevel into tocArray [dotKeyArray]["meta"]["section_level"]
   end repeat
   return tocArray
end fedwikipedia_ConstructPageTocArray


--> Fedwiki | Fetch
-   
function fedwiki_FetchWikipediaPageJson someTitle
   put fedwiki_FetchWikipediaPageArray (someTitle) into pageArray
   put json_FromArray (pageArray) into pageJson
   return pageJson
end fedwiki_FetchWikipediaPageJson

function fedwiki_FetchWikipediaPageArray someTitle
   put wikipedia_FetchPageStuff (someTitle) into someJSON
   put json_ToArray (someJSON) into wikipediaQueryArray
   put fedwiki_ConvertWikipediaQueryArray (wikipediaQueryArray) into pageArray
   return pageArray
end fedwiki_FetchWikipediaPageArray

function fedwiki_FetchWikiMediaPageJson shortImageFile
   put fedwiki_FetchWikiMediaPageArray (shortImageFile) into pageArray
   if pageArray is an array then
      put json_FromArray (pageArray) into pageJSON
      return pageJSON
   else
      return pageArray
   end if
end fedwiki_FetchWikiMediaPageJson

function fedwiki_FetchWikiMediaPageArray shortImageFile
   -- let's get information about the file
   put wikicommons_FetchFileMetadata (shortImageFile) into someJson
   put json_ToArray (someJson) into queryArray
   put queryArray ["query"]["pages"] into pageArray
   put line 1 of keys (pageArray) into pageID -- should only be one line
   put pageArray [pageID]["title"] into imageTitle -- same as shortImageFile
   put pageArray [pageID]["imageinfo"][1] into imageInfoArray
   
   put imageInfoArray ["descriptionurl"] into descriptionURL
   put imageInfoArray ["url"] into imageURL
   put imageInfoArray ["extmetadata"] into extMetArray
   
   put extMetArray ["Categories"]["value"] into imageCategoryIndex
   replace "|" with CR in imageCategoryIndex
   put extMetArray ["ImageDescription"]["value"] into imageDescription
   put extMetArray ["LicenseShortName"]["value"] into licenseDescription
   
   put imageDescription into imageCaption
   html_TopAndTailParagraphTags imageCaption
   fedwiki_AddExternalLink imageCaption, descriptionURL, "wikimedia"
   
   put url imageURL into someImage
   if someImage is empty then return ("Error, image" && imageURL && "is empty")
   set the itemdelimiter to "."
   put item -1 of imageURL into imageType -- bit of a hack
   
   put fedwiki_ConstructNewImagePageArray (imageTitle, someImage, imageType, imageCaption) into pageArray
   return pageArray
end fedwiki_FetchWikiMediaPageArray


--> Fedwiki | Section
-
function fedwikipedia_ExtractShortImageFile droppedHTML
   put xml_ToArray (droppedHTML) into imageArray
   
   put imageArray ["meta"]["img"]["@attributes"] into attributeArray
   put attributeArray ["src"] into thumbURL
   put attributeArray ["srcset"] into srcsetURL
   put attributeArray ["width"] into thumbWidth
   put attributeArray ["height"] into thumbHeight
   
   -- https://upload.wikimedia.org/wikipedia/commons/thumb/7/7f/Bruce_Sterling_at_ARE_2010.jpg/220px-Bruce_Sterling_at_ARE_2010.jpg
   set the itemdelimiter to "/"
   put item -2 of thumbURL into shortImageFile
   return shortImageFile
end fedwikipedia_ExtractShortImageFile


--> Fedwiki | Tweak
-
function fedwikipedia_ConstructSourceArray pageTitle, pageSlug, wikipediaUrl, wikipediaRevisionID, lookUpArray
   put "https://livecode.world/mediawiki/transportAgain" into transportURL
   put the milliseconds into dateTransported
   put "https://github.com/LiveCodeWorld/lib_MediaWiki" into repoURL
   put fedwiki_ConstructCreateSourceArray (transportURL, wikipediaUrl, dateTransported, repoURL, wikipediaRevisionID) into pSourceArray
   -- put infoBoxArray into pSourceArray ["infobox"]
   
   text_Utf8Encode pageSlug -- this fixes bug / crash if page_Slug has non-utf8 characters
   put pageSlug into pSourceArray ["pageSlug"]
   put pageTitle into pSourceArray ["pageTitle"]
   if lookUpArray is an array then
      put lookUpArray into pSourceArray ["link_dictionary"]
   end if
   return pSourceArray
end fedwikipedia_ConstructSourceArray


--> Fedwiki | Convert
-
function fedwiki_ConvertWikipediaQueryArray wikipediaQueryArray
   put wikipediaQueryArray ["query"]["pages"] into pagesArray
   repeat for each key pageID in pagesArray
      put pagesArray [pageID] into pageArray
      
      put pageArray ["title"] into pageTitle
      put pageArray ["extract"] into pageExtract
      put pageArray ["terms"]["description"][1] into pageDescription
      
      put pageArray ["pageImage"] into shortImageFile
      put pageArray ["thumbnail"] into thumbnail
      put pageArray ["images"] into imageArray
      
      exit repeat -- just do one page for now
   end repeat
   
   put fedwiki_ConstructNewPageArray (pageTitle, pageDescription) into pageArray
   pageArray_AddHtml pageArray, pageExtract
   
   wikicommons_AddAllImages pageArray, pageTitle
   
   return pageArray
end fedwiki_ConvertWikipediaQueryArray



command _CleanReferences @wikiText
   -- strips them all for now, and in doing so strips citations as well.
   /*
   -- piping does not work
   put "(<ref" & notClosing & shortClosing & ")" after someReg -- <ref name="dfsa" />
   put "|" after someReg
   put "(" & openingTag & notClosing & closingTag after someReg -- <ref name=...>...</ref> or <ref>...</ref>
   */
   
   local someReg
   
   -- Good regExpressions
   -- put "(<ref" & notClosing & "/>)" after someReg -- <ref name="dfsa" />
   -- put "(<ref>" & notClosing & "</ref>)" after someReg
   
   -- some utility chunks
   put "[^\>]*" into notClosing
   put "\<ref" & notClosing & "\>" into openingTag
   put "\</ref\>)" into closingTag
   put "/\>)" into shortClosing
   -- put "[^\<]*" into notOpening
   
   put "(?mi)" into someReg -- seems to have no effect???
   put "(<ref>" & notClosing & closingTag after someReg -- <ref>...</ref>
   text_StripReg wikiText, someReg
   
   put "(?mi)" into someReg -- seems to have no effect???
   put "(\<ref" & notClosing & shortClosing after someReg -- <ref name="dfsa" />
   text_StripReg wikiText, someReg
   
   put "(?mi)" into someReg -- seems to have no effect???
   put "(" & openingTag & notClosing & closingTag after someReg -- <ref name=...>...</ref> or <ref>...</ref>
   text_StripReg wikiText, someReg
   
   return wikiText
end _CleanReferences


--> Private | Cleaners
-
command _CleanTemplates @wikiText
   /*
   Issue is that no regular expression seems able to detect complex nested templates such as cladograms
   Nor can I find a way to return the text without templates - ie use api to strip them.
   Looks like might be able to get rid of them with the following heuristic.
   - Remove/delete/skip line if first char of indented line begins with:
   - "{{" or "|" or "}}"
   
   Sometimes it starts with "* " and at other times as in "Ant" there are some annoying sentances thrown in the middle :(
   
   The best heuristic seems to be to count "{{" and "}}"
   */
   
   -- now let's iterate whole text
   -- we can try this once for the first header template, or repeat till we don't find a line beginnng with "{{"
   put 0 into openCount
   repeat
      put line_BeginsWith ("{{", wikiText) into startLineNum
      
      if startLineNum = 0 then exit repeat
      repeat
         put word 1 to -1 of line startLineNum of wikiText into nextLine
         switch
            -- maybe faster to repeat code with breaks to avoid checking each condition
            case char 1 to 2 of nextLine = "{{" -- could count
            case char -2 to -1 of nextLine = "}}" -- could count
            case char 1 of nextLine = "|"
            case char 1 of nextLine = "*"
               _AddBracketCount openCount, nextLine
               delete line startLineNum of wikiText
               break
            default
               -- case nextLine is empty
               if openCount > 0 then
                  delete line startLineNum of wikiText
                  next repeat
               else
                  exit repeat
               end if
         end switch
      end repeat
   end repeat
   
   -- strip the simple ones
   _StripTemplates wikitext
   return wikiText
end _CleanTemplates

private command _AddBracketCount @openCount, nextLine
   -- need to add the number of "{{" and subtract the number of "}}" to figure out what to add to total
   -- word 1 to -1 of line startLineNum = nextLine
   
   -- could do for entire body of text
   -- safer inside handler as we are not changing wikiText
   text_StipQuoted nextLine
   
   put text_CountStrings ("{{", nextLine) into addNum
   add addNum to openCount
   
   put text_CountStrings ("}}", nextLine) into subtractNum
   subtract subtractNum from openCount
end _AddBracketCount

private command _StripTemplates @wikitext
   -- now let's try a regular expression
   -- we can try a regular expression
   -- U is for non-greedy, m is for multiline search
   -- put "(?miU)" before someReg -- multiline search
   -- put "[^\}]*" into notThis
   put ".*" into notThis
   put "(\{\{" & notThis & "\}\})" into someReg
   put "(?mi)" before someReg -- multiline search
   --
   text_StripReg wikiText, someReg
end _StripTemplates

private command _CleanTable @wikiText
   -- woud be great to parse and create a fedwiki table?
   
   /*
   Hello
   
   {| class="wikitable sortable" style="float:right; margin:10px;"
   |-
   ! [[Region]] !! Number of<br />species&nbsp;<ref name = HolldoblerWilsonAnts>Hölldobler & Wilson (1990), p. 4</ref>
   |-
   | [[Neotropic ecozone|Neotropics]] ||align="right"| 2162
   |-
   | [[Nearctic ecozone|Nearctic]] ||align="right"| 580
   |-
   | [[Europe]] ||align="right"| 180
   |-
   | [[Africa]] ||align="right"| 2500
   |-
   | [[Asia]] ||align="right"| 2080
   |-
   | [[Melanesia]] ||align="right"| 275
   |-
   | [[Australia]] ||align="right"| 985
   |-
   | [[Polynesia]] ||align="right"| 42
   |}
   
   World
   */
   -- put script_GetCommentAboveMe() into wikiText
   
   put "[^\}]*" into notClosingBracket
   put "(?mi)(\{\|" & notClosingBracket & "\|\})" into someReg
   text_StripReg wikiText, someReg
   return wikiText
end _CleanTable

command _CleanBlockQuotes @wikiText
   replace "<blockquote>" with (CR & "<blockquote>") in wikiText
   replace "</blockquote>" with ("</blockquote>" & CR) in wikiText
end _CleanBlockQuotes

private command _CleanSeeAlso @wikiSectionLine
   -- some start text {{See also|1920s Berlin}} end text 
   -- put script_GetCommentAboveMe() into wikiSectionLine
   
   put "[^\}]+" into untilCurlyBracket
   put "\{\{See also\|(" & untilCurlyBracket & ")\}\}" into someReg
   put "(?m)" before someReg -- multiline search
   repeat
      if matchchunk (wikiSectionLine, someReg, wikiAlsoStart, wikiAlsoEnd) is true then
         put char wikiAlsoStart to wikiAlsoEnd of wikiSectionLine into wikiAlsoLink
         if wikiAlsoStart = 12 then
            put "See also [[" & wikiAlsoLink & "]]" into cleanAlso
         else
            put "(see also [[" & wikiAlsoLink & "]])" into cleanAlso
         end if
         put cleanAlso into char (wikiAlsoStart - 11) to (wikiAlsoEnd + 2) of wikiSectionLine
      else
         exit repeat
      end if
   end repeat
end _CleanSeeAlso

command _CleanComments @wikiText
   -- As an example here is a <!-- Bot generated title --> that needs to be removed.
   -- put script_GetCommentAboveMe() into wikiText
   --
   put "[^\>]*" into notClosingBracket
   --
   put "(\<\!--" into someReg
   put notClosingBracket after someReg
   put "--\>)" after someReg
   --
   put "(?mi)" before someReg -- multiline search
   text_StripReg wikiText, someReg
end _CleanComments


--> HTML | Tidy
-
function html_FetchTidy someURL
   put "curl -s" && someURL && "| tidy -asxhtml --quiet yes --tidy-mark no --show-warnings false --show-errors 0" into someShell
   put shell (someShell) into someXHTML
   return someXHTML
end html_FetchTidy

command html_Tidy @someHtml
   put the tempname & ".html" into inputFile 
   put someHtml into url ("file:" & inputFile)
   
   put "tidy --force-output true --char-encoding utf8 --input-encoding utf8 --output-encoding utf8 --output-xml true -asxml -m" && inputFile into someShell
   put shell (someShell) into shellResult   
   put url ("file:" & inputFile) into someHtml
end html_Tidy

command html_FetchScraped someURL, @tidyXML, @xmlArray, @treeID
   put html_FetchTidy (someURL) into tidyXML
   put revXMLCreateTree (tidyXML, false, true, false) into treeID
   xml_ExtractEncoding tidyXML, versionMatch, theXMLEncoding
   put xml_ArrayFromID (treeID, theXMLEncoding) into xmlArray
end html_FetchScraped


--> Encoding | Text
-
command text_StripReg @wikiText, someReg, pReplaceText
   local refStart, refEnd
   put 0 into indexNum
   repeat
      get matchchunk (wikiText, someReg, sNum, eNum)
      if sNum is not a number then
         put the result into testResult
         -- breakpoint
         exit repeat -- regExp bug???
      end if
      
      if it is true then
         -- this bit not needed (remove for speed)
         -- put char refStart to refEnd of wikiText into someTest
         put sNum into stripResultArray [indexNum]["sNum"]
         put eNum into stripResultArray [indexNum]["eNum"]
         add 1 to indexNum
         --
         -- delete char sNum to eNum of wikiText
         put pReplaceText into char sNum to eNum of wikiText
      else
         exit repeat
      end if
   end repeat
   return stripResultArray
end text_StripReg

command text_StipQuoted @someText
   put "(" & quote & "[^" & quote & "]*" & quote & ")" into quoteReg
   text_StripReg someText, quoteReg, "•••"
end text_StipQuoted

function text_CountStrings someString, someText
   -- not tested
   put 0 into charsToSkip
   put 0 into someCount
   
   repeat
      put offset (someString, someText, charsToSkip) into offsetNum
      if offsetNum = 0 then
         return someCount
      else
         add 1 to someCount
         add offsetNum to charsToSkip
      end if
   end repeat
end text_CountStrings
   
command text_Utf8Encode @someText
   -- put unidecode(uniencode(someText),"UTF8") into someText
   put utf8_Encode (someText) into someText
end text_Utf8Encode


--> Encoding | XML
-
function html_StripTags pHtml
   -- returns the meaningful text from a web page
   local tRegex,tPrevText
   -----
   replace return with space in pHtml
   replace numtochar(13) with empty in pHtml
   replace tab with empty in pHtml
   -----
   put replacetext(pHtml,"(?Usi)<SCRIPT.*</SCRIPT>","") into pHtml
   put replacetext(pHtml,"(?Usi)<STYLE>.*</STYLE>","") into pHtml
   put replacetext(pHtml,"(?Usi)<\?.*\?>","") into pHtml
   -----
   replace "&nbsp;" with space in pHtml
   replace "<BR>" with return in pHtml
   replace "<p>" with return in pHtml
   -----
   put  "<[^><]*>" into tRegex
   put replacetext(pHtml,tRegex,"") into pHtml
   put replacetext(pHtml,tRegex,"") into pHtml
   -----
   repeat until tPrevText is pHtml
      put pHtml into tPrevText
      put replacetext(pHtml," +",space) into pHtml
      put replacetext(pHtml,"^ ","") into pHtml
   end repeat
   -----
   replace (space & return) with return in pHtml
   replace (return & space) with return in pHtml
   filter pHtml without empty
   -----
   put html_ReplaceEntities (pHtml) into pHtml
   -----
   return pHtml
end html_StripTags

function html_ReplaceEntities someHtml
   replace "&quot;" with quote in someHtml
   
   -- Replace all the html entities that Rev understands with their character equivalents.
   put html_EntityList() into htmlEntities
   put html_EntityCharNums() into entityCharNums
   
   repeat with ii = 1 to the number of items of htmlEntities
      put item ii of entityCharNums into entityCharNum
      put numtochar(entityCharNum) into someChar
      replace item ii of htmlEntities with someChar in someHtml
   end repeat
   return someHtml
end html_ReplaceEntities

function html_EntityList
   /*
   Special characters (whose ASCII value is greater than 127) are encoded as HTML entities.
   Revolution recognizes the following named entities:
   */
   get "&Aacute;,&aacute;,&Acirc;,&acirc;,&acute;,&AElig;,&aelig;,&Agrave;,&agrave;,&Aring;,&aring;,&Atilde;,&atilde;,&Auml;,&auml;,&brvbar;,&Ccedil;,&ccedil;,&cedil;,&cent;,&copy;,&curren;,&deg;,&divide;,&Eacute;,&eacute;,&Ecirc;,&ecirc;,&Egrave;,&egrave;,&ETH;,&eth;,&Euml;,&euml;,&frac12;,&frac14;,&frac34;,&gt;,&Iacute;,&iacute;,&Icirc;,&icirc;,&iexcl;,&Igrave;,&igrave;,&iquest;,&Iuml;,&iuml;,&laquo;,&lt;,&macr;,&micro;,&middot;,&nbsp;,&not;,&Ntilde;,&ntilde;,&Oacute;,&oacute;,&Ocirc;,&ocirc;,&Ograve;,&ograve;,&ordf;,&ordm;,&Oslash;,&oslash;,&Otilde;,&otilde;,&Ouml;,&ouml;,&para;,&plusmn;,&pound;,&raquo;,&reg;,&sect;,&shy;,&sup1;,&sup2;,&sup3;,&szlig;,&THORN;,&thorn;,&times;,&Uacute;,&uacute;,&Ucirc;,&ucirc;,&Ugrave;,&ugrave;,&uml;,&Uuml;,&uuml;,&Yacute;,&yacute;,&yen;,&yuml"
   put "&amp;" & comma before it
   return it
end html_EntityList

function html_EntityCharNums
   get "231,135,229,137,171,174,190,203,136,129,140,204,139,128,138,3,130,141,252,162,169,219,161,214,131,142,230,144,233,143,15,19,232,145,12,11,14,62,234,146,235,148,193,237,147,192,236,149,199,60,248,181,225,202,194,132,150,238,151,239,153,241,152,187,188,175,191,205,155,133,154,166,177,163,200,168,164,4,7,5,6,167,18,21,16,242,156,243,158,244,157,172,134,159,17,20,180,216"
   put "38" & comma before it
   return it
end html_EntityCharNums


--> Encoding | XML
-
function xml_ToArray pXML, pTranslateKeyBoolean, pStoreEncodedAs, pUseValueKey, pGoodXML
   -- was "xml_ConvertXmlToArray"
   local theArray, theResult, theRootNode, treeID
   local theXMLEncoding
   
   ## Create an XML tree from XML text
   put pGoodXML is true into pGoodXML
   put revCreateXMLTree (pXML, pGoodXML, true, false) into treeID
   
   if treeID is an integer then
      xml_ExtractEncoding pXML, versionMatch, theXMLEncoding
      
      ## Now convert to array.
      ## The 1st dimension has one key which is the name of the root node.
      put revXMLRootNode(treeID) into theRootNode
      if theRootNode is not empty and not (theRootNode begins with "xmlerr,") then
         put xml_ConvertXMLNodeToArray (treeID, theRootNode, theXMLEncoding, pStoreEncodedAs, pUseValueKey, pTranslateKeyBoolean) into theArray [theRootNode]
      end if
      
      revDeleteXMLTree treeID
   end if
   return theArray
end xml_ToArray

function xml_ArrayFromID treeID, theXMLEncoding, pStoreEncodedAs, pUseValueKey, pTranslateKeyBoolean
   xml_ExtractEncoding pXML, versionMatch, theXMLEncoding
   put revXMLRootNode (treeID) into theRootNode
   if theRootNode is not empty and not (theRootNode begins with "xmlerr,") then
      put xml_ConvertXMLNodeToArray (treeID, theRootNode, theXMLEncoding, pStoreEncodedAs, pUseValueKey, pTranslateKeyBoolean) into theArray [theRootNode]
   else
      return empty
   end if
   return theArray
end xml_ArrayFromID

private function xml_ConvertXMLNodeToArray pTreeID, pNode, pXMLTreeEncoding, pStoreEncodedAs, pUseValueKey, pTranslateKeyBoolean
   /*
   Helper function for xml_ToArray.
   Converts an XML node to a multi-dimensional array.-- Calls itself recursively.
   */
   local theArrayA, theAttributes, theChildNode, theKey
   
   ## Look for attributes of the node. Store as array in "@attributes" key
   put revXMLAttributes (pTreeID, pNode, tab, cr) into theAttributes
   if theAttributes is not empty then
      put xml_EncodeString (theAttributes, pXMLTreeEncoding, pStoreEncodedAs) into theAttributes
      split theAttributes by cr and tab -- create array
      put theAttributes into theArrayA ["@attributes"]
   end if
   
   ## Look for children nodes.
   set the itemdelimiter to slash
   put revXMLFirstChild (pTreeID, pNode) into theChildNode
   if theChildNode is empty or theChildNode begins with "xmlerr," then
      put xml_EncodeString (revXMLNodeContents (pTreeID, pNode), pXMLTreeEncoding, pStoreEncodedAs) into theValue
      if word 1 to -1 of theValue is empty and the keys of theArrayA is not empty then
         ## Empty node that has attributes
         return theArrayA
      else if pUseValueKey then
         ## Force value into @value
         put theValue into theArrayA ["@value"]
         return theArrayA
      else
         ## Single Node with value: Return value. Attributes are ignored.
         return theValue
      end if
   else
      ## Child nodes were found. Recursively call self and store result in array.
      repeat while theChildNode is not empty and not (theChildNode begins with "xmlerr,")
         put the last item of theChildNode into theKey
         
         /*
         if pTranslateKeyBoolean is true then
            put the array_KeyTranslation [theKey] of the target into theKey
            if theKey is empty then next repeat -- this can repeat forever
         end if 
         */
         
         put xml_ConvertXMLNodeToArray (pTreeID, theChildNode, pXMLTreeEncoding, pStoreEncodedAs, pUseValueKey, pTranslateKeyBoolean) into theArrayA [theKey]
         put revXMLNextSibling (pTreeID, theChildNode) into theChildNode
      end repeat
      
      return theArrayA
   end if
end xml_ConvertXMLNodeToArray

private function xml_EncodeString pString, pInEncoding, pOutEncoding
   -- Helper function for converting the encoding of strings when converting to and from XML.
   
   ## convert utf-8 to utf8 for uniencode/decode
   replace "-" with empty in pInEncoding
   replace "-" with empty in pOutEncoding
   
   if pInEncoding is not empty then
      -- if pOutEncoding is empty then pString will be converted to the current platform encoding
      return unidecode(uniencode(pString, pInEncoding), pOutEncoding)
   else
      if pOutEncoding is not empty then
         -- if pInEncoding is empty then pString is assumed to be in the current platform encoding
         return unidecode(uniencode(pString, pInEncoding), pOutEncoding)
      else
         return pString
      end if
   end if
end xml_EncodeString

command xml_ExtractEncoding someXML, @versionMatch, @xmlEncoding
   put matchtext(someXML, "<\?xml (.*)encoding=" & quote & "(.*)" & quote & "\?>", versionMatch, xmlEncoding) into theResult
   if xmlEncoding is empty then put "utf-8" into xmlEncoding
   return theResult
end xml_ExtractEncoding


--> Deps
-
function line_BeginsWith startLineChars, someLines, pLinesToSkip
   -- probably a faster way of doing this
   -- find the first line starting with...
   -- set the wholematches to false
   -- put lineoffset ("{{", wikiText) into startLineNum -- though I could do this with wholematches
   
   if pLinesToSkip is a number then delete line 1 to pLinesToSkip of someLines
   
   put CR & startLineChars into startlineChars
   put CR & someLines into testContainer
   put offset (startlineChars, testContainer) into startCharNum
   if startCharNum is 0 then
      return 0
   else
      get char 2 to (startCharNum + the number of chars of startLineChars - 1) of testContainer
      put the number of lines of it into lineNum
      return lineNum
   end if
end line_BeginsWith
   
command file_SaveTempText someText, pFileExt
   if pFileExt is empty then put "txt" into pFileExt
   put the tempname & "." & pFileExt into someFile
   put someText into url ("file:" & someFile)
   return someFile
end file_SaveTempText

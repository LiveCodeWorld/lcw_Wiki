script "lib_FedWikiPedia"
--> Metadata
-
name: lib_FedWikiPedia
type: library
version: 1.4
copyright: David Bovill
licence:  GPLv3
deps: lib_MediaWiki, lib_Fedwiki

/*
These handlers interface between Wikipedia and Fedwiki.
Contains clean functions for wikipedia import into fedwiki.

== TO DO ==
Everything at the bottom of this code is in process of being moved to it's own library - lib_Encoding
*/


--> Remove
-
function fedwikipedia_ConstructSourceArray pageTitle, pageSlug, wikipediaUrl, wikipediaRevisionID, lookUpArray
   -- for old Transport Again
   put "https://livecode.world/mediawiki/transportAgain" into transportURL
   put the milliseconds into dateTransported
   put "https://github.com/LiveCodeWorld/lib_MediaWiki" into repoURL
   put fedwiki_ConstructCreateSourceArray (transportURL, wikipediaUrl, dateTransported, repoURL, wikipediaRevisionID) into pSourceArray
   -- put infoBoxArray into pSourceArray ["infobox"]
   
   text_Utf8Encode pageSlug -- this fixes bug / crash if page_Slug has non-utf8 characters
   put pageSlug into pSourceArray ["pageSlug"]
   put pageTitle into pSourceArray ["pageTitle"]
   if lookUpArray is an array then
      put lookUpArray into pSourceArray ["link_dictionary"]
   end if
   return pSourceArray
end fedwikipedia_ConstructSourceArray


--> Working On
-  
function mw_SummaryPageArray mwTitle, pLang, pAddToc
   -- used by "summary" of "mediawiki.lc"
   -- see also "fedwiki_FetchWikipediaSummaryPageArray"
   -- very similar to "mediawiki_FetchPageArray"
   /*   
   -- consider using "put mediawiki_FetchWikiText("Ant")"
   -- also uses old "mediawiki_FetchSummaryDescription" to fetch multiple first lines
   -- as restbase won't do that except in html
   -- would be great to enable links in first paragraphs like "wikicommons_ConstructSectionPageArray" does
   -- using sectionnum = 0 does not work with "wikicommons_ConstructSectionPageArray"
   */
   
   put mediaWiki_ConstructSlug (mwTitle) into mwSlug
   put mediawiki_FetchSummaryDescription (mwSlug, true, true, pLang) into firstSection -- uses old stuff anyway to get more than one line
   put restbase_FetchSummaryArray (mwSlug, pLang) into summaryArray -- fetch some metadata
   put mediawiki_ConstructSummaryPageArray (firstSection, summaryArray) into pageArray
   
   # Add TOC Button
   put "http://rest.livecode.world/mediawiki/tocButtons" into postURL
   put empty into bgColor
   put mwTitle into valueArray ["pageTitle"]
   pageArray_AddButton pageArray, "Table of Contents", postURL, valueArray, "", "right", bgColor
   
   if pAddToc is true then
      # Add TOC Section
      put mediawiki_FetchMarkdownTOC (mwSlug) into markdownTOC
      if markdownTOC is not empty then
         put "# See also" && CR&CR before markdownTOC
         pageArray_AddMarkdown pageArray, markdownTOC
      end if
   end if
   --
   pageArray_StripJournal pageArray
   pageArray_AddWikipediaCred pageArray, mwTitle, mwSlug
   --
   return pageArray
end mw_SummaryPageArray


--> Wikipedia
-
command wikipedia_DownloadImages pageTitle
   -- maximum height 1080
   -- mediawiki_NormaliseTitle pageTitle, pageTitle, pageID, pLang
   -- mediawiki_NormaliseTitle pageTitle, pageTitle, pageID
   
   put restbase_ListImageUrls (pageTitle) into imageUrls
   if imageUrls is empty then
      -- let's guess it is Spansih
      put restbase_ListImageUrls (pageTitle, "es") into imageUrls
   end if
   
   if imageUrls is empty then return empty
   
   put dropbox_MediaWikiImageFolder (pageTitle) into someFolder
   folder_CreateNested someFolder
   
   set the itemdelimiter to slash
   repeat for each line someURl in imageUrls
      set the cursor to busy
      put urldecode (item -1 of someURl) into shortFile
      put "binfile:" & someFolder & shortFile into localURL
      put url someURl into url localURL
   end repeat
   
   return imageUrls
end wikipedia_DownloadImages

function dropbox_MediaWikiImageFolder pageTitle
   put dropbox_Folder ("Imagine Media Installation - Footage") & pageTitle & slash into someFolder
   return someFolder
end dropbox_MediaWikiImageFolder

function dropbox_Folder pShortFolder
   put "/Users/fortyfoxes/Dropbox/" into someFolder
   if pShortFolder is not empty then
      put pShortFolder & slash after someFolder
   end if
   return someFolder
end dropbox_Folder


--> Wikipedia | List
-
function wikicommons_ListImages shortImageFile
   -- this does not seem to return good lists of images
   -- lot's of dodgy characters = spam? in file names etc?
   -- api.php?action=query&list=allimages&ailimit=5&aifrom=Albert&aiprop=dimensions|mime [try in ApiSandbox]
   -- put "Albert Einstein" into fromName
   -- put "IndustrialSymbiosisWasteHeatExchange.png" into shortImageFile
   
   put "action=query&list=allimages&format=json" into urlPattern
   -- put "&titles=Image:[[shortImageFile]]" after urlPattern
   put "&titles=[[shortImageFile]]" after urlPattern
   put wikicommons_GetApiStem() & merge (urlPattern) into restURL
   
   put mediawiki_FetchJSON (restURL) into someJSON
   return someJSON
end wikicommons_ListImages

function wikipedia_ListPages titleString, pNearMatch, pLang
   -- if pNearMatch is true then the title is not returned properly capitalised
   put wikipedia_FetchSearchJson (titleString, pNearMatch, pLang) into someJSON
   put json_ToArray (someJSON) into queryResultArray
   -- display_Array queryResultArray
   
   put queryResultArray ["query"]["search"] into searchResultArray
   repeat with indexNum = 1 to item 2 of the extents of searchResultArray
      put searchResultArray [indexNum]["title"] into pageTitle
      put pageTitle & CR after pageTitles
   end repeat
   delete char -1 of pageTitles
   sort lines of pageTitles
   return pageTitles
end wikipedia_ListPages

function fedwiki_FetchWikipediaSummaryPageArray pageSlug, pApiStem
   -- use "mw_SummaryPageArray" instead
   -- see also "fedwikipedia_FetchSectionSummaryPageArray"
   -- need to make a "parse" call to get TOC
   local pSectionNum
   
   mediawiki_FetchPageParseInfo pageSlug, pageDescription, pageTitle, pageImageFile, infoBoxArray, pSectionNum, pApiStem
   put the result into sandboxArray
   put fedwikipedia_ConstructPageTocArray (sandboxArray) into tocArray
   
   fedwikipedia_ConstructTocLookupArray tocArray, lookUpArray, markdownTOC
   if lookUpArray is an array then put "http://rest.livecode.world/mediawiki/importSection" into lookUpArray ["default"]["transport"]
   
   -- need to make a "query" call to get a nice clean text
   mediawiki_FetchPageQueryInfo pageSlug, simplePageDescription, pageTitleAgain, shortImageFile, wikipediaRevisionID, true, true, pApiStem
   put the result into sandboxPageArray
   
   if shortImageFile is empty then
      put pageImageFile into shortImageFile
   end if
   
   put wikipedia_ConstructUrl (pageSlug) into wikipediaUrl
   -- put sandboxPageArray ["fullurl"] into wikipediaUrl
   put sandboxPageArray ["coordinates"] into coordArray
   
   -- now let's construct the page
   set the itemdelimiter to "."
   if item -1 of shortImageFile is "webm" then
      put wikicommons_ConstructVideoPageArray (pageTitle, pageDescription, wikipediaUrl, shortImageFile, pSourceArray) into fedwikiPageArray
   else
      put wikicommons_ConstructTwoImagePageArray (pageTitle, pageDescription, shortImageFile, wikipediaUrl, pSourceArray) into fedwikiPageArray
   end if
   
   if coordArray is an array then
      -- add a map?
   end if
   
   put "# See also" & CR & markdownTOC into markdownSection
   pageArray_AddMarkdown fedwikiPageArray, markdownSection
   
   pageArray_StripJournal fedwikiPageArray
   return fedwikiPageArray
end fedwiki_FetchWikipediaSummaryPageArray


--> Mediawiki | Fetch | Alternative
-
command mediawiki_FetchPageParseInfo pageSlug, @pageDescription, @pageTitle, @shortImageFile, @infoBoxArray, pSectionNum, pApiStem
   -- Uses "parse" to extract wiktext sections. Lot's of fiddling with text. Messy as it returns lots of page templates that need cleaning. But we get wiki links
   
   if pSectionNum is empty then
      put sandbox_ConstructSectionFragment (pageSlug) into sandBoxFragment
   else
      put sandbox_ConstructSectionFragment (pageSlug, pSectionNum) into sandBoxFragment
   end if
   
   put sandbox_FetchArray (sandBoxFragment, pApiStem) into sandboxArray
   
   put sandboxArray ["parse"] into pageArray
   put pageArray ["title"] into pageTitle
   put pageArray ["wikitext"]["*"] into pageDescription
   mediawiki_CleanPageDescription pageDescription, infoBoxArray
   if infoBoxArray is an array then
      put infoBoxArray ["image"] into shortImageFile
   else
      put pageArray ["images"] into imageArray
      put mediawiki_ExtractFirstInterestingImage (imageArray) into shortImageFile
   end if
   return sandboxArray
end mediawiki_FetchPageParseInfo


--> Fedwiki | Clean
-
/*
These handlers are for parsing, and cleaning mediawiki stypled wikitext, to turn it into a form that is useful for Fedwiki.
*/

command fedwikipedia_CleanInternalLinks @wikiSectionLine
   -- used by "wikicommons_AddSectionToPageArray"
   
   -- strips out image files as well (carefull)
   -- some starting text [[Machtergreifung|came to power]] something [[Real Link|well a second one]] at end
   -- Berlin was devastated by [[Bombing of Berlin in World War II|bombing raids]], fires and street battles during 
   
   put "\[\[([^\]]+)\|([^\]]+)\]\]" into someReg
   put "(?m)" before someReg -- multiline search
   repeat
      if matchchunk (wikiSectionLine, someReg, wikiLinkStart, wikiLinkEnd, linktextStart, linktextEnd) is true then
         -- [[Rhaeto-Romans|Romansh language]] => Rhaeto-Romans ([[Romansh language]])
         -- but does not fix section links [[Rhaeto-Romans#Stuff]] and other stuff
         
         put char wikiLinkStart to wikiLinkEnd of wikiSectionLine into wikiLink
         put char linktextStart to linktextEnd of wikiSectionLine into wikiLinkText
         --
         -- put wikiLinkText && "([[" & wikiLink & "]])" into cleanedLink -- a bit too cluttered
         put "[[" & wikiLink & "]]" into cleanedLink
         --
         put cleanedLink into char (wikiLinkStart - 2) to (linktextEnd + 2) of wikiSectionLine
      else
         exit repeat
      end if
   end repeat
end fedwikipedia_CleanInternalLinks


--> FedwikiPedia | Fetch
-   
function fedwiki_FetchWikipediaSummaryPageJson pageSlug, pApiStem
   put fedwiki_FetchWikipediaSummaryPageArray (pageSlug, pApiStem) into fedwikiPageArray
   put json_FromArray (fedwikiPageArray) into pageJSON
   return pageJSON
end fedwiki_FetchWikipediaSummaryPageJson

function fedwikipedia_ConstructPageTocArray sandboxArray
   put sandboxArray ["parse"]["sections"] into sectionArray
   put sectionArray [1]["fromTitle"] into tocTitle
   
   repeat for each key tocNum in sectionArray
      put sectionArray [tocNum]["level"] into tocLevel
      put sectionArray [tocNum]["line"] into tocLine
      put sectionArray [tocNum]["number"] into tocNumber
      
      put tocNumber into tocArray ["index"][tocLine]["dot_number"]
      put tocNum into tocArray ["index"][tocLine]["section_number"]
      
      put tocNumber into dotKeyArray
      split dotKeyArray by "."
      put tocLine into tocArray [dotKeyArray]["meta"]["title"]
      put tocNumber into tocArray [dotKeyArray]["meta"]["dot_number"]
      put tocNum into tocArray [dotKeyArray]["meta"]["section_number"]
      put tocLevel into tocArray [dotKeyArray]["meta"]["section_level"]
   end repeat
   return tocArray
end fedwikipedia_ConstructPageTocArray


--> Fedwiki | Fetch
-   
function fedwiki_FetchWikipediaPageJson someTitle
   put fedwiki_FetchWikipediaPageArray (someTitle) into pageArray
   put json_FromArray (pageArray) into pageJson
   return pageJson
end fedwiki_FetchWikipediaPageJson

function fedwiki_FetchWikipediaPageArray someTitle
   put wikipedia_FetchPageStuff (someTitle) into someJSON
   put json_ToArray (someJSON) into wikipediaQueryArray
   put fedwiki_ConvertWikipediaQueryArray (wikipediaQueryArray) into pageArray
   return pageArray
end fedwiki_FetchWikipediaPageArray

function fedwiki_FetchWikiMediaPageJson shortImageFile
   put fedwiki_FetchWikiMediaPageArray (shortImageFile) into pageArray
   if pageArray is an array then
      put json_FromArray (pageArray) into pageJSON
      return pageJSON
   else
      return pageArray
   end if
end fedwiki_FetchWikiMediaPageJson

function fedwiki_FetchWikiMediaPageArray shortImageFile
   -- let's get information about the file
   put wikicommons_FetchFileMetadata (shortImageFile) into someJson
   put json_ToArray (someJson) into queryArray
   put queryArray ["query"]["pages"] into pageArray
   put line 1 of keys (pageArray) into pageID -- should only be one line
   put pageArray [pageID]["title"] into imageTitle -- same as shortImageFile
   put pageArray [pageID]["imageinfo"][1] into imageInfoArray
   
   put imageInfoArray ["descriptionurl"] into descriptionURL
   put imageInfoArray ["url"] into imageURL
   put imageInfoArray ["extmetadata"] into extMetArray
   
   put extMetArray ["Categories"]["value"] into imageCategoryIndex
   replace "|" with CR in imageCategoryIndex
   put extMetArray ["ImageDescription"]["value"] into imageDescription
   put extMetArray ["LicenseShortName"]["value"] into licenseDescription
   
   put imageDescription into imageCaption
   html_TopAndTailParagraphTags imageCaption
   fedwiki_AddExternalLink imageCaption, descriptionURL, "wikimedia"
   
   put url imageURL into someImage
   if someImage is empty then return ("Error, image" && imageURL && "is empty")
   set the itemdelimiter to "."
   put item -1 of imageURL into imageType -- bit of a hack
   
   put fedwiki_ConstructNewImagePageArray (imageTitle, someImage, imageType, imageCaption) into pageArray
   return pageArray
end fedwiki_FetchWikiMediaPageArray


--> Fedwiki | Section
-
function fedwikipedia_ExtractShortImageFile droppedHTML
   put xml_ToArray (droppedHTML) into imageArray
   
   put imageArray ["meta"]["img"]["@attributes"] into attributeArray
   put attributeArray ["src"] into thumbURL
   put attributeArray ["srcset"] into srcsetURL
   put attributeArray ["width"] into thumbWidth
   put attributeArray ["height"] into thumbHeight
   
   -- https://upload.wikimedia.org/wikipedia/commons/thumb/7/7f/Bruce_Sterling_at_ARE_2010.jpg/220px-Bruce_Sterling_at_ARE_2010.jpg
   set the itemdelimiter to "/"
   put item -2 of thumbURL into shortImageFile
   return shortImageFile
end fedwikipedia_ExtractShortImageFile


--> Fedwiki | Convert
-
function fedwiki_ConvertWikipediaQueryArray wikipediaQueryArray
   put wikipediaQueryArray ["query"]["pages"] into pagesArray
   repeat for each key pageID in pagesArray
      put pagesArray [pageID] into pageArray
      
      put pageArray ["title"] into pageTitle
      put pageArray ["extract"] into pageExtract
      put pageArray ["terms"]["description"][1] into pageDescription
      
      put pageArray ["pageImage"] into shortImageFile
      put pageArray ["thumbnail"] into thumbnail
      put pageArray ["images"] into imageArray
      
      exit repeat -- just do one page for now
   end repeat
   
   put pageArray_Construct (pageTitle, pageDescription) into pageArray
   pageArray_AddHtml pageArray, pageExtract
   
   wikicommons_AddAllImages pageArray, pageTitle
   
   return pageArray
end fedwiki_ConvertWikipediaQueryArray


--> HTML | Tidy
-
function html_FetchTidy someURL
   if the platform is "server" then
      put "curl -s" && someURL && "| tidy -asxhtml --quiet yes --tidy-mark no --show-warnings false --show-errors 0" into someShell
   else
      put "curl -s" && someURL && "| /usr/local/bin/tidy -asxhtml --quiet yes --tidy-mark no --show-warnings false --show-errors 0" into someShell
   end if
   put shell (someShell) into someXHTML
   return someXHTML
end html_FetchTidy

command html_Tidy @someHtml
   put the tempname & ".html" into inputFile 
   put someHtml into url ("file:" & inputFile)
   
   put "tidy --force-output true --char-encoding utf8 --input-encoding utf8 --output-encoding utf8 --output-xml true -asxml -m" && inputFile into someShell
   put shell (someShell) into shellResult   
   put url ("file:" & inputFile) into someHtml
end html_Tidy

command html_FetchScraped someURL, @tidyXML, @xmlArray, @treeID
   put html_FetchTidy (someURL) into tidyXML
   put revXMLCreateTree (tidyXML, false, true, false) into treeID
   xml_ExtractEncoding tidyXML, versionMatch, theXMLEncoding
   put xml_ArrayFromID (treeID, theXMLEncoding) into xmlArray
end html_FetchScraped


--> Encoding | Text
-
command text_StripReg @wikiText, someReg, pReplaceText
   local refStart, refEnd
   put 0 into indexNum
   repeat
      get matchchunk (wikiText, someReg, sNum, eNum)
      if sNum is not a number then
         put the result into testResult
         -- breakpoint
         exit repeat -- regExp bug???
      end if
      
      if it is true then
         -- this bit not needed (remove for speed)
         -- put char refStart to refEnd of wikiText into someTest
         put sNum into stripResultArray [indexNum]["sNum"]
         put eNum into stripResultArray [indexNum]["eNum"]
         add 1 to indexNum
         --
         -- delete char sNum to eNum of wikiText
         put pReplaceText into char sNum to eNum of wikiText
      else
         exit repeat
      end if
   end repeat
   return stripResultArray
end text_StripReg

command text_StipQuoted @someText
   put "(" & quote & "[^" & quote & "]*" & quote & ")" into quoteReg
   text_StripReg someText, quoteReg, "•••"
end text_StipQuoted

function text_CountStrings someString, someText
   -- not tested
   put 0 into charsToSkip
   put 0 into someCount
   
   repeat
      put offset (someString, someText, charsToSkip) into offsetNum
      if offsetNum = 0 then
         return someCount
      else
         add 1 to someCount
         add offsetNum to charsToSkip
      end if
   end repeat
end text_CountStrings
   
command text_Utf8Encode @someText
   -- put unidecode(uniencode(someText),"UTF8") into someText
   put utf8_Encode (someText) into someText
end text_Utf8Encode


--> Encoding | XML
-
function html_StripTags pHtml
   -- returns the meaningful text from a web page
   local tRegex,tPrevText
   -----
   replace return with space in pHtml
   replace numtochar(13) with empty in pHtml
   replace tab with empty in pHtml
   -----
   put replacetext(pHtml,"(?Usi)<SCRIPT.*</SCRIPT>","") into pHtml
   put replacetext(pHtml,"(?Usi)<STYLE>.*</STYLE>","") into pHtml
   put replacetext(pHtml,"(?Usi)<\?.*\?>","") into pHtml
   -----
   replace "&nbsp;" with space in pHtml
   replace "<BR>" with return in pHtml
   replace "<p>" with return in pHtml
   -----
   put  "<[^><]*>" into tRegex
   put replacetext(pHtml,tRegex,"") into pHtml
   put replacetext(pHtml,tRegex,"") into pHtml
   -----
   repeat until tPrevText is pHtml
      put pHtml into tPrevText
      put replacetext(pHtml," +",space) into pHtml
      put replacetext(pHtml,"^ ","") into pHtml
   end repeat
   -----
   replace (space & return) with return in pHtml
   replace (return & space) with return in pHtml
   filter pHtml without empty
   -----
   put html_ReplaceEntities (pHtml) into pHtml
   -----
   return pHtml
end html_StripTags

function html_ReplaceEntities someHtml
   replace "&quot;" with quote in someHtml
   
   -- Replace all the html entities that Rev understands with their character equivalents.
   put html_EntityList() into htmlEntities
   put html_EntityCharNums() into entityCharNums
   
   repeat with ii = 1 to the number of items of htmlEntities
      put item ii of entityCharNums into entityCharNum
      put numtochar(entityCharNum) into someChar
      replace item ii of htmlEntities with someChar in someHtml
   end repeat
   return someHtml
end html_ReplaceEntities

function html_EntityList
   /*
   Special characters (whose ASCII value is greater than 127) are encoded as HTML entities.
   Revolution recognizes the following named entities:
   */
   get "&Aacute;,&aacute;,&Acirc;,&acirc;,&acute;,&AElig;,&aelig;,&Agrave;,&agrave;,&Aring;,&aring;,&Atilde;,&atilde;,&Auml;,&auml;,&brvbar;,&Ccedil;,&ccedil;,&cedil;,&cent;,&copy;,&curren;,&deg;,&divide;,&Eacute;,&eacute;,&Ecirc;,&ecirc;,&Egrave;,&egrave;,&ETH;,&eth;,&Euml;,&euml;,&frac12;,&frac14;,&frac34;,&gt;,&Iacute;,&iacute;,&Icirc;,&icirc;,&iexcl;,&Igrave;,&igrave;,&iquest;,&Iuml;,&iuml;,&laquo;,&lt;,&macr;,&micro;,&middot;,&nbsp;,&not;,&Ntilde;,&ntilde;,&Oacute;,&oacute;,&Ocirc;,&ocirc;,&Ograve;,&ograve;,&ordf;,&ordm;,&Oslash;,&oslash;,&Otilde;,&otilde;,&Ouml;,&ouml;,&para;,&plusmn;,&pound;,&raquo;,&reg;,&sect;,&shy;,&sup1;,&sup2;,&sup3;,&szlig;,&THORN;,&thorn;,&times;,&Uacute;,&uacute;,&Ucirc;,&ucirc;,&Ugrave;,&ugrave;,&uml;,&Uuml;,&uuml;,&Yacute;,&yacute;,&yen;,&yuml"
   put "&amp;" & comma before it
   return it
end html_EntityList

function html_EntityCharNums
   get "231,135,229,137,171,174,190,203,136,129,140,204,139,128,138,3,130,141,252,162,169,219,161,214,131,142,230,144,233,143,15,19,232,145,12,11,14,62,234,146,235,148,193,237,147,192,236,149,199,60,248,181,225,202,194,132,150,238,151,239,153,241,152,187,188,175,191,205,155,133,154,166,177,163,200,168,164,4,7,5,6,167,18,21,16,242,156,243,158,244,157,172,134,159,17,20,180,216"
   put "38" & comma before it
   return it
end html_EntityCharNums


--> Encoding | XML
-
function xml_ToArray pXML, pTranslateKeyBoolean, pStoreEncodedAs, pUseValueKey, pGoodXML
   -- was "xml_ConvertXmlToArray"
   local theArray, theResult, theRootNode, treeID
   local theXMLEncoding
   
   ## Create an XML tree from XML text
   put pGoodXML is true into pGoodXML
   put revCreateXMLTree (pXML, pGoodXML, true, false) into treeID
   
   if treeID is an integer then
      xml_ExtractEncoding pXML, versionMatch, theXMLEncoding
      
      ## Now convert to array.
      ## The 1st dimension has one key which is the name of the root node.
      put revXMLRootNode(treeID) into theRootNode
      if theRootNode is not empty and not (theRootNode begins with "xmlerr,") then
         put xml_ConvertXMLNodeToArray (treeID, theRootNode, theXMLEncoding, pStoreEncodedAs, pUseValueKey, pTranslateKeyBoolean) into theArray [theRootNode]
      end if
      
      revXMLDeleteTree treeID
   end if
   return theArray
end xml_ToArray

function xml_ArrayFromID treeID, theXMLEncoding, pStoreEncodedAs, pUseValueKey, pTranslateKeyBoolean
   xml_ExtractEncoding pXML, versionMatch, theXMLEncoding
   put revXMLRootNode (treeID) into theRootNode
   if theRootNode is not empty and not (theRootNode begins with "xmlerr,") then
      put xml_ConvertXMLNodeToArray (treeID, theRootNode, theXMLEncoding, pStoreEncodedAs, pUseValueKey, pTranslateKeyBoolean) into theArray [theRootNode]
   else
      return empty
   end if
   return theArray
end xml_ArrayFromID

private function xml_ConvertXMLNodeToArray pTreeID, pNode, pXMLTreeEncoding, pStoreEncodedAs, pUseValueKey, pTranslateKeyBoolean
   /*
   Helper function for xml_ToArray.
   Converts an XML node to a multi-dimensional array.-- Calls itself recursively.
   */
   local theArrayA, theAttributes, theChildNode, theKey
   
   ## Look for attributes of the node. Store as array in "@attributes" key
   put revXMLAttributes (pTreeID, pNode, tab, cr) into theAttributes
   if theAttributes is not empty then
      put xml_EncodeString (theAttributes, pXMLTreeEncoding, pStoreEncodedAs) into theAttributes
      split theAttributes by cr and tab -- create array
      put theAttributes into theArrayA ["@attributes"]
   end if
   
   ## Look for children nodes.
   set the itemdelimiter to slash
   put revXMLFirstChild (pTreeID, pNode) into theChildNode
   if theChildNode is empty or theChildNode begins with "xmlerr," then
      put xml_EncodeString (revXMLNodeContents (pTreeID, pNode), pXMLTreeEncoding, pStoreEncodedAs) into theValue
      if word 1 to -1 of theValue is empty and the keys of theArrayA is not empty then
         ## Empty node that has attributes
         return theArrayA
      else if pUseValueKey then
         ## Force value into @value
         put theValue into theArrayA ["@value"]
         return theArrayA
      else
         ## Single Node with value: Return value. Attributes are ignored.
         return theValue
      end if
   else
      ## Child nodes were found. Recursively call self and store result in array.
      repeat while theChildNode is not empty and not (theChildNode begins with "xmlerr,")
         put the last item of theChildNode into theKey
         
         /*
         if pTranslateKeyBoolean is true then
            put the array_KeyTranslation [theKey] of the target into theKey
            if theKey is empty then next repeat -- this can repeat forever
         end if 
         */
         
         put xml_ConvertXMLNodeToArray (pTreeID, theChildNode, pXMLTreeEncoding, pStoreEncodedAs, pUseValueKey, pTranslateKeyBoolean) into theArrayA [theKey]
         put revXMLNextSibling (pTreeID, theChildNode) into theChildNode
      end repeat
      
      return theArrayA
   end if
end xml_ConvertXMLNodeToArray

private function xml_EncodeString pString, pInEncoding, pOutEncoding
   -- Helper function for converting the encoding of strings when converting to and from XML.
   
   ## convert utf-8 to utf8 for uniencode/decode
   replace "-" with empty in pInEncoding
   replace "-" with empty in pOutEncoding
   
   if pInEncoding is not empty then
      -- if pOutEncoding is empty then pString will be converted to the current platform encoding
      return unidecode(uniencode(pString, pInEncoding), pOutEncoding)
   else
      if pOutEncoding is not empty then
         -- if pInEncoding is empty then pString is assumed to be in the current platform encoding
         return unidecode(uniencode(pString, pInEncoding), pOutEncoding)
      else
         return pString
      end if
   end if
end xml_EncodeString

command xml_ExtractEncoding someXML, @versionMatch, @xmlEncoding
   put matchtext(someXML, "<\?xml (.*)encoding=" & quote & "(.*)" & quote & "\?>", versionMatch, xmlEncoding) into theResult
   if xmlEncoding is empty then put "utf-8" into xmlEncoding
   return theResult
end xml_ExtractEncoding


--> Deps
-
function line_BeginsWith startLineChars, someLines, pLinesToSkip
   -- probably a faster way of doing this
   -- find the first line starting with...
   -- set the wholematches to false
   -- put lineoffset ("{{", wikiText) into startLineNum -- though I could do this with wholematches
   
   if pLinesToSkip is a number then delete line 1 to pLinesToSkip of someLines
   
   put CR & startLineChars into startlineChars
   put CR & someLines into testContainer
   put offset (startlineChars, testContainer) into startCharNum
   if startCharNum is 0 then
      return 0
   else
      get char 2 to (startCharNum + the number of chars of startLineChars - 1) of testContainer
      put the number of lines of it into lineNum
      return lineNum
   end if
end line_BeginsWith
   
command file_SaveTempText someText, pFileExt
   if pFileExt is empty then put "txt" into pFileExt
   put the tempname & "." & pFileExt into someFile
   put someText into url ("file:" & someFile)
   return someFile
end file_SaveTempText
